#+html_head: <link rel="stylesheet" type="text/css" href="org.css"/>
#+title: Cuboids Manifesto
#+options: toc:nil
#+options: html-postamble:nil


| Author   | [[http://tigue.com][John Tigue]]           |
| Version  | 2020-11-04-1         |
| License  | CC0 1.0              |

* Abstract

This document proposes refresh modifications and novel extentions to
the bossDB volumentric database. The envisioned platform has been
given the working title of Cuboids.

During the MICrONS Project, the bossDB (a.k.a., "the Boss") code has
proven to scale to petabytes of data. This is very impressive as it is
unusual for a codebase to be so extremely stress tested in its initial
deployment. In order to position the Boss to be the open source core
backend repository for the next generation of neuroscientific
experiments, the codebase should be streamlined for ease of
administration and to address feedback from the original users.

The initial goal of the Cuboids project is an architectural update of
bossDB to leverage the latest AWS serverless innovations thereby
simplifying deployment and maintenance of the existing system, while
maintaining the existing bossDB surface APIs. The subsequent goal is
to use the streamlined bossDB as the repository component of a cloud
service which can do more than just store and visualize data.

The resulting Cuboids platform is to be a "whole product" which
recapitulates the multi-system architecture of the MICrONS project but
with all tasks -- storage, analysis, visualization, etcetera --
performable within a Cuboids instance. The first process to be moved
into Cuboids is the code used during MICrONS for reconstruction of
neurons via image recognition machine learning inference. After that
the system will be an agile platform for further innovations. Examples
include inference by Google's Flood Filling Networks, training machine
learning models, and classification by cell type.

#+attr_html: :width 80%
[[./images/logos/cuboids_lettermark_bw.png]]

#+TOC: headlines 3h

* Introduction

The [[https://bossdb.org/][bossDB]] is a [[https://en.wikipedia.org/wiki/Volume_rendering][volumetric]] database capable of storing petabytes of
data generated from microscopic imaging of neurological
tissue. Colloquially, bossDB is also known as "the Boss" and in this
document both terms are interchangable. 

Initial development of the Boss was funded via grants from [[https://www.iarpa.gov/][IARPA]] under
[[https://www.iarpa.gov/index.php/research-programs/microns][the MICrONS project]]. The resulting source code -- spanning multiple
repositories -- is available under the [[https://www.apache.org/licenses/LICENSE-2.0][Apache 2.0]] open source license.

The modified bossDB codebase proposed herein is designed to be the
repository component of a platform which has been given the working
title of Cuboids. Cuboids builds atop existing work by doubling down
on the current AWS lock-in of bossDB, in the interests of streamlining
the architecture in order to make it easier to administer. This part
of the proposal is simply "better, faster, cheaper" thinking which
does not change the high-level feature set of the repository.

The original bossDB codebase evidences a valid and shrewd
architectural style which values vendor neutral open source. Yet to
deal with the sporatic, massive volume and velocity of data being
throw off by electron microscopes, an AWS serverless engine was
adopted. This worked well but resulted in a mixed architectural style
which is more complex than currently necessary.  Cuboids aims to
resolve the complexity by going all-in on AWS lock-in by jettisoning
any existing components which can be replaced by equivalent fully
managed AWS services.

Beyond the aforementioned architectural refresh, Cuboids aims to build
significant analysis functionality close around the repository. Given
the current architecture of the Boss, image analysis has to be
performed by separate external systems, requiring the transfer of
mountains of data. Cuboids builds on the previous bossDB work such
that analysis of the data within a bossDB repository can be performed
close to where the data resides.

This functional expansion can be accomplished by shepparding the
existing serverless boosted architecture towards a serverless-first
design. In 2015, when work on the Boss was started, a serverless-first
architecture simply could not be build because AWS has not yet built
out enough serverless products. The term "serverless-first" refers in
part to the minimizing of self-managed services. More importantly, the
balance is to swing from the current Boss architecture of a
non-serverless frame with a serverless engine to one which builds out
from the serverless core and then adds on long running, self-managed
compute instances only as necessited by technical or financial
concerns.

Peta-scale datasets are unlikely to ever migrate from where they are
initially reposited. Recognizing this and the fact that the existing
AWS-only codebase is currently managing petabytes of data in AWS, this
manifesto accepts the reality of single vendor lock-in with regards to
cloud provider platform and so drinks deeply from the AWS Kool-Aid
pitcher by moving towards less servers and more fully managed
services.

#+attr_html: :width 25%
[[./images/aws_pitcher.png]]

* The history of the Boss

The Boss was written to be the repository for data generated by the
MICrONS Project. The data consists of movies, static images, and the
results of machine learning based analysis of the movies and images.

The funding came out of IARPA. The code is open source and [[https://github.com/jhuapl-boss/spdb/blob/master/LICENSE.md][licensed]] in
a commercially friendly manner, that is under the Apache 2.0 license.

** The MICrONS Project

In the interest of brevity, MICrONS is expounded upon only minimally in
this document. See the companion document, the MICrONS Project
Primer, for more details.

Tersely, the Boss -- created by JHU/APL -- was used in both Phase One and Phase
Two of MICrONS. Phase One was a "small" trail run of the main task
which occurred in Phase Two. The resulting EM image sets were 40 terabyte
versus 2.5 petabyte, respectively, a ratio of 1:62. Phase One involved
three separate groups of organizations competitively trying out to be
awarded the grant for the Phase Two work.  The winning team which
moved on to Phase Two consisted of [[https://alleninstitute.org/][The Allen Institute]], [[https://seunglab.org/][the Seung Lab
at Princeton University]], and [[https://toliaslab.org/][the Tolias Lab]] at Baylor University.

The technical architecture of the data system of the MICrONS project
centers around the Boss. The design enabled coordination across the
organizations participating in MICrONS.  The following diagram
illustrates part of the flow of data in and out of the Boss repository
during Phase Two.

#+attr_html: :width 65%
[[./images/microns_phase_two_dataflow.jpg]]

Four organization each focused on their particular area of expertise:
- JHU/APL wrote the Boss repository code and supporting tools
- Baylor University performed functional imaging experiments on a live mouse
- The Allen Institute electron microscope (EM) scanned that mouse's brain
- The Seung Lab wrote the machine learning code to reconstruct the imaged neurons

The functional imaging generated many movies of neural activities
taken while the mouse was performing in various learning experiments.
Next the structural imaging involved scanning a cubic millimeter of
the mouse's brain via a industrialized EM workflow which generated 2.5
petabyte of TIFF image files. (The distinction between structural
imaging and functional imaging is analogous to that between the map of
a road network and recordings of traffic over the network,
respectively.) Finally, the reconstruction work involved downloading
the EM images and mining them via custom built machine learning
algorithms -- deep CNNs navigating through massive volumes of 3D data
to recognize neurons down to indiviual synapses. (Not shown here is the
work to coregister the functional and structural data.)

Click on the following image to see a video which demonstrates what
such reconstruction algorithms do. The colored map on the 2D plane
slice is the output of a segmentation algorithm, an intermediate
product of the full reconstruction process:

#+macro: imglnk @@html:<a href="$1"><img src="$2"></a>@@
{{{imglnk(https://youtu.be/X4eVmSxTZ8Y,images/reconstruction_demo.jpg)}}}

Via [[https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html][Improving Connectomics by an Order of Magnitude]], Google AI Blog, 2018-07


** The Boss technical architecture

The docs describe the Boss as:
#+begin_quote
The Boss is a large-scale spatial database service for storing
multi-dimensional neuroimaging data and associated voxel annotations
that was created for the IARPA MICrONS program. The database is
designed to support three dimensional, multi-channel, and time series
source data and annotations at various bit depths. The Boss was
designed to live in the cloud and has a tiered storage architecture to
balance cost and performance, auto-scaling capabilities, and a
high-bandwidth ingest process.
#+end_quote   
   
In its current form the Boss architecture can be considered a hybrid
of two styles. The foundational frame consists of non-serverless,
platform neutral, open source machinery originally assembled for the
NeuroData project. The JHU/APL developers were already familiar with
these solid technologies and it made sense to reapply them in the Boss
codebase. Within that frame was built out a turbo boosted engine of serverless
tech. These decisions were made for valid reasons back in 2015 and time has
shown the resulting codebase has served its original purpose well.

The following diagram is from the original bossDB pre-print. In order
to break it down, the yellow and red backgound boxes with labels in
bold have been added as annotations. The yellow box highlights the
serverless core of the architecture.  The red boxes define three
non-serverless sub-systems which were built out of platform neutral,
open source machinery. The colors were chosen to correspond to the
argument presented in later sections: keep the yellow, jettison the
red.

#+attr_html: :width 75%
[[./images/bossdb_delta_components.png]]

The core engine of BossDB is the spatial database, repo name
[[https://github.com/jhuapl-boss/spdb][jhuapl-boss/spdb]]. This service features the usual suspects found in
AWS severless machines: Lambda, S3, SQS, DynamoDB, Step Functions,
etc.


The other three sub-systems in red are proven non-serverless, open
source technologies: Bastion, Vault, Keycloak, etc.
- The upper left red zone is the REST interface
- The Single Sign-on subsystem is in the upper right red zone
- The lower reg zone can be replace with AWS Secrets Manager

Those three sub-systems are not particularly interesting, domain
specific machinery. Nonetheless, the functionality they provide
amounts to table stakes for a modern, mature application. In AWS
marketing terminology they are [[https://jeremybower.com/articles/undifferentiated-heavy-lifting-2-0.html]["undifferentiated heavy lifting"]]
support machinery. JHU/APL simply reapplied solid codebases they were
already familiar with while getting on with the task of building a
petascale volumetric spatial database.

Sundry adopted AWS services are enumerated in the bottom dash boxed
region. These services are applicable to both serverless and container
based architectures and will be kept in Cuboids.

Note: the hybrid nature of the Boss architecture is not a result of
the multi-team social structure of the MICrONS Project. On the other
hand, the fact that the original Boss cannot perform image recognition
internally is indeed a reflection of the social structure. The
decisions which lead to this state can be seen as a wisely chosen
limiting of the scope of development for the initial Boss. But, as
will be argued herein, it is time to change that.

From an external perspective, the resulting architectural structure is
still sound. Cuboids will maintain the existing Boss HTTP APIs to the
repository component and only modify its internal structure. Thereby
analysis tools written atop Cuboids will be able to interface with
multiple bossDB repositories, including the one wherein the MICrONS
data resides.

* Realtechnik of cloud architecting

Significant but non-core parts of the bossDB codebase (the red boxes
in the previous diagram) are simply deployments of existing open
source components which just so happens to be platform neutral,
non-serverless tech. Vendor neutrality is a good thing but Cuboids
consciously jettisons that feature in the interests of accellerating
innovation via improved developer velocity and reduced devops
workload. Perfectly good open-source component of the Boss will be
replaced with equivalent AWS fully managed services.

This document is entitled "The Cuboids Manifesto" in reference to the
"[[https://www.youtube.com/watch?v=s1AiBi5gf1s][manifesto]]" documents that circulate within the Swift programming
community (and it is a bit long). This type of document serves the
purpose of introducing a proposal which is larger in scope than a
single, small, specific proposal. This document follows that
lighthearted nomenclature, in order to highlight that there is a
[[https://www.ribbonfarm.com/2012/08/16/realtechnik-nausea-and-technological-longing/][realtechnik]] "philosophy" driving the decisions behind what is proposed
herein.

There are two main assumptions to the argument:
- The original Boss codebase can only be deployed on AWS
- Very large datasets (in particular, the MICrONS data on AWS) are rather immobile

As to the former assumption, normally it is ideal if open source code
is platform vendor neutral. Yet falling short of such ideals there are
still benefits to the open source way, for example, innovation shared
throughout a community of users. The Boss, given its AWS serverless
engine, is very much not vendor neutral.  For example, the Boss makes
use of AWS Step Functions. Unfortunately there is currently no
equivalent abstraction available which works on other cloud platforms
(which seems odd but c'est la vie).

Given the AWS technologies already adopted in the bossDB codebase,
Cuboids accepts and runs with the reality of AWS lock-in, probably in
perpetuity. It is arguably a bit perverse for open source to be tuned
up just for a single commercial cloud platform, but the hypothesis
herein is that by doing so it will make it much easier for other
organizations to deploy BossDB, thereby speeding up the diffusion of
innovation.

The latter assumption acknowledges the inertia of peta-scale
datasets. The MICrONS data is already in AWS S3. Other smaller
datasets are also accreting in Boss deploys on AWS. So code
which analyzes these datasets yet only runs on AWS is not a major
negative. Of course, this too is gradually leading to more AWS lock
in. Other cloud vendors are missing out on an emerging industry
sector. Obviously it is not the biggest sector in terms of users but this
is heading towards exo-scale datasets which is not insignificant.

Datasets on such a scale as that of the MICrONS data form a
gravitational well inducing system architectures wherein data
processing happens as close to the data as possible; it is desirable
to minimise massive data trasfers. Although the existing bossDB APIs
can be used to fetch raw data for analysis on systems external to the
Boss, Cuboids intends to extend the Boss platform such that data
analysis can be performed within the system. Quoting Francis Bacon:
#+BEGIN_QUOTE
Mahomet cald the Hill to come to him. And when the Hill stood still,
he was neuer a whit abashed, but said; If the Hill will not come to
Mahomet, Mahomet wil go to the hil. [sic]
#+END_QUOTE

#+attr_html: :width 400px
[[./images/mohammed_and_the_mountain.jpg]]

The two above assumptions drive the logic of this document. This
proposal argues to dive headlong down the slippery slope of AWS
lock-in. This recommended direction may seem counterintuitive and even
distasteful given the platform vendor neutral, open source ecosystem
out of which the Boss grew.

Nonetheless, although the technological lead which AWS currently
enjoys over the other cloud platform vendors [[https://cloudwars.co/amazon/inside-amazon-aws-no-longer-jeff-bezos-growth-engine/][is slipping]], it does not
appear that a cross-platform serverless version of bossDB is possible
at this time, given the state of those cloud platform offerings
competing with AWS (e.g. a lack of an AWS Step Functions equivalent
for Google Cloud Platform).

Therefore, it is argued herein that the mixed architecture is
retarding the [[https://en.wikipedia.org/wiki/Diffusion_of_innovations][diffusion of innovation]], especially with regards to that
theory's criteria of "complexity" and "trialability." So, perhaps we
should just accept that this codebase will always be locked in to AWS
and drink their Kool Aid in order to lighten the load.

As a reality check on this AWS serverless enthusiasm, there is a
decent 2020-10 article, [[https://www.infoq.com/articles/serverless-stalled/][Why the Serverless Revolution Has Stalled]],
which enumerates situations where serverless is not yet living up to
its promise. The Boss as a use case of serverless technology actually
passes almost all the checkpoints enumerated in that article: for
example, all the code is written in Python (i.e. a language well
supported by AWS Lambda), it is greenfield, and the entire app was
explicitly designed to run on a cloud platform from the get go. The
single concern raised which the Boss is guilty of vendor lock-in but
as argued herein, that is acknowledged and accepted.

So, the Cuboids code will be very locked to AWS as the vendor
platform, which is a rather ironic form of open source. It is assumed
that these changes to bossDB are significantly disruptive enough that
PRs back to bossDB would not be worth the hassle to the repo's
maintainers. But maybe some chuncks of code will go upstream, say, the
heaviside replacement code. Perhaps upstream would be interested in
the Step Function that works with chunkflow for neuron reconstruction;
being novel that part would not involve major changes to existing code
but that could be seen as feature creap. Regardless, the Cuboids
codebase will remain open source, intentionally maintaining the Apache
2.0 license is the hopes of building an community around the codebase.

* Architectural updates

The first stage of Cuboids development is to be an architectural
refresh of the existing Boss codebase. 

Actually, the initial development work will be to simply stand up a
Boss instance using the existing codebase. That will be needed for
testing backward compatibility. Surely the deployment experience will
cause refinements to what is proposed in this document. That
deployment experience will be documented. Perhaps logging that will
prove useful to other adminstrators of Boss deploys.

The bossDB codebase was [[https://github.com/jhuapl-boss/boss/graphs/contributors][started in 2015]], which was early days for AWS
serverless, nevermind using such young technology to scale to
petabytes of data. In the interim, Lambda has become more
sophisticated (e.g., layers) and AWS has gotten around to providing
more fully managed services such as API Gateway, Cognito, and AWS
Secrets Manager. The life of a Boss administrator could involve less
hassles if the Boss were to adopt those innovations.

** Overview

The architectural refresh is motivated by abstract goals and those
resolve to a set of specific coding tasks.  The abstract goals are to
effect the following changes.

| Before                     | After                         |
|----------------------------+-------------------------------|
| Hybrid architecture        | Serverless first architecture |
| Some self-managed services | More fully managed services   |
| Complex deployment         | Simplified deployment         |

The main goal of the architectural refresh is to reduce the complexity
resulting from the hybrid architecture by moving towards a
serverless-first architecture. Most of the specific coding tasks
revolve around replacing self-managed sub-systems with equivalent
fully managed AWS services. Simply keeping the current serverless
core architecture but rewriting those components which can be replaced
with equivalent AWS service will make bossDB easier to deploy and
maintain.

Note that the goals include *not* significantly modifying
the features of the Boss repository. Specifically, the surface RESTful
APIs are to maintain backward compatibility. Any distinctly novel
functionality to be introduced in Cuboids which is not present in the
Boss codebase will be constructed outside the Boss repository
component.

The refurbishments to be performed can be illustrates by further
marking up the Boss high level architecture diagram from earlier.
These annotations are color coded according to a stop light
red-yellow-green color scheme. The red boxes are to be replace with
the AWS services named in green. The red boxes are labeled by their
roles in black bold text. Those same roles are to be resatisfied with
the named (in green) AWS fully managed services.

#+attr_html: :width 100%
[[./images/boss_high_level_changes.png]]

In the above diagram, the parts of the architecture to be refurbished are:
- The RESTful interface machinery: to be handled by Amazon API Gateway
- The Single-Sign On (SSO) machinery: to be migrated AWS Cognito
- The secrets keeping machinery: to be replaces with AWS Secrets Manager
- The Redis cache: to be managed by Amazon ElastiCache

These sub-systems to be modified are simply necessary support
infrastructure -- undifferentiated, off-the-shelf app infrastructure,
not the bespoke domain specific innovations contained in the core serverless
engine of the spatial database. The code to be replaced is perfectly
good code; that is proven platform independent, scalable open
source. But as pre-serverless tech it requires self management and
related infrastructures.

Besides the above mentioned machinery refurbishments there is also a
tooling library, [[https://github.com/jhuapl-boss/heaviside][heaviside]], which is to be replaced with AWS
equivalent tooling, that is, the Python AWD CDK. Heaviside is another
instance of JHU/APL banging out a wheel before AWS got around to
recreating an equivalent. Heaviside does its job just fine but the
goal here is to set the Boss up for the next ten years. While in Rome
do as the Romans, and in AWS the CDK is the way to do what heaviside
does. AWS sees their CDKs as important tools. Might as well benefit
from the freely available tooling supported by AWS developer
resource.

In the following before-and-after juxtaposition diagram, the image on
the left is the raw image taken from the bossDB preprint, with the
serverless core highlighted in yellow. The image on the right is the
envisioned streamlined and serverless-first Boss. Notice how it has
been reduced to a completely generic serverless architecture. That
clearly illustrates the argument being made but has reduced the
diagram to where it contains no specific detail. Going into such
detail is beyond the scope of this document.

#+attr_html: :width 100%
[[./images/arch_before_and_after.png]]

** RESTful interface machinery

API Gateway
- The upper left red zone can be replaced by API Gateway etc.

nginx is used to run some Python, a django app using rest_framework to
handle REST API calls coming in over the public network. Cuboids can
come in and out, optionally being compressed via HTTP content
negotiation.

It is a goal to maintain backwards compatibility of the
interface. This refurbishment is purely about simplyfying the
machinery which implements the interface, not about changing the user
experience of the interface.

One way to test for compatibiilty is to have one test client which can
work with both the existing Boss and Cuboids simply by changing the
end-point URL.  The existing Boss integration test suites should prove
useful.

API Gateway's main purpose is to provide HTTP interfaces to AWS
Lambda. Since the Boss core is Lambda-based serverless it is only
natural to use API Gateway. This is an instance of an undifferentiated
component which the Boss needed but for which there was no available
AWS service at the time of initial development of the Boss (actually
API Gateway had just been released at the time but was passed over,
seemingly).

The code for the boss API machinery and schema is in the repository
named [[https://github.com/jhuapl-boss/boss][jhuap-boss/boss]].


** Single sign-on
- Keep [[https://docs.theboss.io/docs/authentication][the Boss auth REST APIs]] but replace the machinery with Cognito
  - It is important to precisely mimic these so that Cuboids hosted apps can get their data from MICrONS if necessary.
  - Use API Gateway to host/expose a Swift file that mimics the Boss auth APIs
    - And compute needed will be some trivial Lambda's (which who
      knows may well already exist somewhere)
  - Existing OIDC Indentity Providers (such as the one at auth.bossdb.io) can be added to a Cognito User Pool
    - This means Cuboids can know who the existing bossdb.io user are and can create Cognito user roles for them while in Cuboids
- Boss APIs look to be pretty much proxies to Cognito APIs
  - List User: https://api.theboss.io/v1/sso/user/:user_name
  - List users groups
  - Create a user
  - Delete a user
  - Assign user roles
  - Revoke user roles
  - Create a group
  - Delete a group
  - List users groups
  - List group members
  - Assign user to group
  - Remove user from group
  - Assign group to resource with a permission set
  - Create resource
  - Delete resource  
- Perhaps modify or use the code for boss-oidc
  - https://github.com/jhuapl-boss/boss-oidc
- Moving to Cognito will simplify per tenant billing and logging.
- The upper right red zone can be replaced by Cognito
  #+begin_quote
  We use the open source software package Keycloak as an identity
  provider to manage users and roles. We created a Django OpenID
  Connect plugin to simplify the integration of services with the SSO
  provider.
  ...

  Our identity provider server intentionally runs in- dependently from
  the rest of bossDB system, forc- ing the bossDB API to authenticate
  just like any other SSO integrated tool or application, and making
  fu- ture federation with other data archives or authenti- cation
  systems easy.

  The Keycloak server is deployed in an auto-scaling group that sits
  behind an Elastic Load Balancer.
  #+end_quote

An envisioned use case of Cuboids is to run analysis on parts of the MICrONS dataset.
This implies that Cuboids should maintain the
architecural structure of the single sign-on machinery including the
APIs. Behind the SSO APIs in Cuboids will be Lambdas that work with
Cognito. This way analysis in Cuboids can fetch data from the MICrONS bossDB
as easily as it's internal bossDB.

Data fetched from the MICrONS repo would go into the local cache, and stick around
for the duration of analysis?

Cuboids allows "real time" exploratory analysis of bossDB data. The
serverlessness makes it fast and easy to provision and compute for the
analysis. The analysis happens in Jupyter. Write little chuncks that start up analysis in Cuboids  There needs to be ways of
seeing costs from within Jupyter i.e. a dashboard in Jupyter.

- Want to be able to have a high res billing system.
  - Cognito makes that easier
  - Want a University to deploy yet be able to bill distinct departments
  - Want multi-tenant SaaS, which is similar to the university and departments

- Consider a security interface or delegator
  - core serverless engine would only talk to the interface/delegator
  - then security could be config to a Cognito provider
  - Or maybe even a dummy provider i.e. let anybody, do anything.
    - simpler management :)

- Perhaps there is already a bridge between Cogniton and whatever they are using for SSO
  - this way could still respect whatever they have going on but core code only talks Cognito
  - i.e. the pluggable interface IS Cognito.
    - So, dummy security would be a Cognito provider that says "whatevs" to anything.

** Secrets
- The lower reg zone can be replace with AWS Secrets Manager
- Existing
  - Vault servers are Secrets which store their info in Consul
  - Consul Servers are for key/value store

** Cache machinery

[ *TODO:* - How does bossDB spin up Redis? Is it fully managed? The
diagram seems to imply it is, with the asterix. ]
   
The above tasks involve migrating generice, support infrastructure
from self-managed code to fully managed AWS services. In contrast,
refurbishing the cache machinery is a modification to the serverless
core i.e. the code in the spdb repository. Nonetheless this task is
another instance of streamlining by migrating to fully managed
services.
   
The Read/Write Cache is part of the core engine, enabling higher
performance. It is a Redis cache running on a cluster of EC2
instances. It is arguably serverless because the cache is wired up to
Lambdas which sync (read and write) the data in the Redis in-memory
cache with the persistant store, S3.

The one component in sbdb that is not currently serverless is the
cache manager instance which is an EC2 instance. It will be removed
and its functionality subsumed into the equivalent fully managed
service, [[https://aws.amazon.com/elasticache/redis/fully-managed-redis/][Amazon ElastiCache for Redis]]. 

There will still be a Redis cluster with EC2 instances containing the
in-memory data but that will be autoscaling and fully managed by
ElastiCache. The goal is for any Redis machinery to be reduced to
CloudFormations config of supporting services used in a serverless
application.

AWS's variant of Redis provides integration with other
AWS services such as Amazon EC2, Amazon CloudWatch, AWS CloudTrail,
and Amazon SNS. Such integrations can be useful if going all-in on
building out for AWS only.

The main code is in the [[https://github.com/jhuapl-boss/spdb][jhuapl-boss/spdb]] repo. The most significant
files is [[https://github.com/jhuapl-boss/spdb/blob/master/spdb/spatialdb/rediskvio.py][rediskvio.py]] which is all about using Redis as a cache for
cuboids. There is also [[https://github.com/jhuapl-boss/spdb/blob/master/spdb/spatialdb/state.py][CacheStateDB class]]: "to implement the Boss
cache state database and associated functionality...Database is a
redis instance." The code that generates the CloudFormation for the Redis
cache machinery is in the boss-manage repo, in
[[https://github.com/jhuapl-boss/boss-manage/blob/master/cloud_formation/configs/redis.py][cloudformation/configs/redis.py]].

** Infrastructure as code tooling

Finally, there is another technology of the Boss which will be replace
with an equivalent AWS technology. Unlike the above tasks this is not
about services. Rather, JHU/APL's developer tool, Heaviside, it to be
replaced with AWS CDK.
   
The core of the Boss is a sophisticated serverless application which
relies heavily on AWS StepFunctions.  Actually writing StepFunctions
in the [[https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html][Amazon States Language]] -- raw JSON -- is an ugly hassle. To
address this inconvenience, JHU/APL created an infrastructure-as-code tool they call
[[https://github.com/jhuapl-boss/heaviside][Heaviside]], a "Python library and DSL for working with AWS
StepFunctions."

Heaviside is another example of JHU/APL (specifically, Derek Pryor)
inventing tech needed in order to sanely build AWS serverless
applications, in this case apps involving StepFunctions.  Eventually AWS
got around to releasing their own equivalent technology, AWS CDK.

So, a decision presents itself: continue to build the core high level
logic of Boss internal programs atop a one-off tooling library by a
small team or take the hit of rewriting the existing
Heaviside code, porting it to the now dominant standard tool, AWS CDK?
Migrating to AWS CDK would position the Boss to reap the benefits of
working with a more mature implementation of a tech which is currently
diffused throughout a much larger community of developers with the weight
of AWS support behind it.

The decision for Cuboids is to replace all existing Heaviside code
with equivalent CDK code, then move forward atop the standard AWS
technology. AWS CDK is available for multiple programming
languages. The Boss is written in Python so it is only logical to use
the Python AWS CDK.

In the boss-manage repository, [[https://github.com/jhuapl-boss/boss-manage/tree/master/cloud_formation/stepfunctions][18 StepFunctions]] already exist, written
to be "compiled" by Heaviside. These are to be rewritten to use the
standard tool for such things: AWS CDK.

* Architectural extensions
** Additional services as serverless-first Step Functions

- Want to write bossDB based programs/experiments which are StepFunctions
- Say, convolving some Vaa3D plug-ins over a volume
- Say, countless (2,2,2) => (1,1,1) fast pyramid builder
- Dont want to do that on heaviside which is why Heaviside is to be
  jettisoned and Cuboids is to move forward using CDK instead.

The main AWS service than enables serverless-first is Step Functions,
which has already been adopted into the Boss machinery. There are
already at least [[https://github.com/jhuapl-boss/boss-manage/tree/master/cloud_formation/stepfunctions][18 StepFunctions]]. For example, the Downsample service
involves [[https://github.com/jhuapl-boss/boss-manage/blob/master/cloud_formation/stepfunctions/resolution_hierarchy.hsd][a Step Function]]. 

Doing so will set Cuboids on a path to be
more of a "whole product" for microscopy workflows rather than, as
with the Boss, essentially a repository accessed by external analysis
tools.

Note that "serverless-first" does not exclude non-serverless
processes. Step Functions can orchestrate both types. Step Functions
can invoke serverless Lambdas, and other non-serverless processes can
interact with Step Functions as Activity workers performing
Tasks. From a Step Function's perspective, the former are "pushed to"
and the latter "pull from." This means that Step Functions based
services can benefit from massively scalable serverless tech as
implemented by AWS Lambdas yet also work with legacy code and/or
processes which are too big or long running to be executed within AWS
Lambda. For example, reconstruction can be orchestrated using the
Seung Lab's [[https://github.com/seung-lab/chunkflow][chunkflow]], which can be wrapped as a Activity which polls
Step Functions for reconstruction jobs to perform. This is how Cuboids
will recapitulate the MICrONS multi-system workflow.

The above point is not obvious from the Boss codebase. The Boss has
lots of StepFunctions but they deal primarily with Lambdas, not long
running processes as task Activities. [ *TODO:* any?]
   
- [[https://aws.amazon.com/blogs/compute/implementing-serverless-manual-approval-steps-in-aws-step-functions-and-amazon-api-gateway/][Implementing Serverless Manual Approval Steps in AWS Step Functions and Amazon API Gateway]]
- [[https://aws.amazon.com/about-aws/whats-new/2017/02/amazon-api-gateway-integration-with-aws-step-functions/][Amazon API Gateway Integration with AWS Step Functions]]
- [[https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-api-gateway.html][Creating a Step Functions API Using API Gateway]]

This is what would make for a serverless first architecture. The state
that bridges both serverless and non-serverless process is maintained
in a Step Function.

Deconvolution is a another Step Functions based service.

So, say some novel is to be added, say cuboid segmentation. To the
outside world the service will manifest as new methods added to the
REST APIs. Inside AWS, The HTTP messaages containing Boss REST
requests are handled by AWS API Gateway (APIGW) which initiates a step
function instance to run a job. So states will be lambdas; some states
might be long runing Activities, an EC2 instance which runs chunkflow
processes.

Chuckflow is a job systems for processing cuboids of data where DCNNs
are run over small volumes (sub-cuboids) to perform ML based volume
segmentation to individual neurons etc. There are masters which
initiate jobs and there are workers who each do their little task. The
workers and masters could be any machines but are normally EC2
instances. At its core, their interaction is orchesrated by an SQS queue.

The workers could easily be Lambdas. That's when it becomes hybrid but
not in a serverless-first fashion. Next would be to wrap the existing
chuckflow master as a Step Funtions activity. Still not
serverless-first but could be a component in a StepFunction. So
experiment first with Lambda chunkflow workers, then if that goes well
figure out how to take the master code and shoehorn that into Lamba
somehow. That would be the full serverless implementation achieved in
stages.

Cuboids could also participate in old-school chunkflow systems. This
could be simply as Lambdas deployed to work with a stock chunkflow
system. Another option, Cuboids would be a worker in the main chunkflow system
and would delegate internally to Lambdas in an internal SQS doing the
same thing as chunkflow, or just passing through to individual Lambdas is small enough?]

This chuckflow example is used because it illustrates how Step
Functions can involve both Lamba and EC2 instances. That's why it's
worth implement services as Step Functions. Step Functions Activities
(i.e. the EC2 instances) are the escape valve override mechanism for
processes that cannot be made to work under the limitations of AWS
serverless e.g. Lambda's runtime limit of 15 minutes. The Step
Functions programs that treat Lambdas and Activities as equivalents
(i.e. just state nodes in the main state machine) is the
serverless-first way to have a hybrid architecture. Say, some CNN
training needs to be done and the data to train on is in a Boss
repository. By framing task as a Step Function, both Lambda-base and
EC2-based servers can be mixed. [ *TODO:* Is this the best example?
Why would one want to do that?  Ingest and train? ]

As argued above, is would be desirable for the compute of analysis to
happen close to the data. Ideally that would happen within the same
VPN, but simply between AWS VPNs is good enough.

This section clarifies the distinction between the surface APIs of the
Boss -- which are the HTTP API interface through which external
programs interact with the Boss -- and the internal service component
APIs upon which can be built programs that run within the Boss.

Ergo, the compute of work of purely digital analysis and scientific
experiments should be performed within the same VPC as the Boss
repository wherein the data to be analyzed resides.  The base
framework for such programs already exists within the Boss codebase in
the form of the AWS Step Functions already written in the codebase.
For example, the downsampling (Kleissa Hider 2017, Section 2.3.7):

#+begin_quote
To allow users to quickly assess, process, and interact with their
data, we need to iteratively build a resolution hierarchy for each
dataset by downsampling the source data. This is a workflow that is
run infrequently and on-demand, and needs to scale from gigabytes to
petabytes of data. We developed a serverless architecture built on AWS
Step Functions to manage failures and track process state. AWS Lambda
is used to perform the underlying image processing in an
embarrassingly parallel, scalable fashion. This approach allows us to
minimize resource costs while scaling on-demand in a fully-automated
paradigm
#+end_quote

Step Functions *are* programs -- programs that just so happen to have
explicitly defined state machines. (Step Functions bring state to the
innately stateless Lambdas.)  The states, Lambdas and Activities, are
the program modules which get assembled into a Step Functions based
program.  The Downsampler is the poster childe program written atop
the Boss platform.

Cuboids will continue to build more Step Functions based programs that
run within the platform. Neuron reconstruction will be the first one.

(Footnote: although AWS Step Functions are an AWS only service, it is
clear that the other cloud providers will offer equivalent
services. It may well be that there will be an open source equivalent
which can run on multiple cloud platforms, rather than being a fully
manage service. The obvious place for that is [[http://serverless.com][The Serverless
Framework]]. If multiple proprietary service arise, a thin abstraction
around them would suffice. This is core cloud architecture tech
begging for a vendor neutral implementation. It would be foundational
for multi-cloud solutions. Step Functions based programs are composed
of Lambdas and Activities, the innately stateless states in a
(stateful) Step Functions based program. They can be mentally modeled
as HTTP API'd services. There is nothing AWS specific about this
model. It can become Multi-cloud FOSS. So, ironically by proceeding
forward with Step Function, Cuboids may well be planning ahead to
remove the vendor lock in. But this is long term speculation. It does
not have to happen and for now is a digression.)

*** Example: cell type classification via CMN
    
- [[https://www.nature.com/articles/s41467-019-10836-3][Learning cellular morphology with neural networks]]
  - [ ] This is Princeton work
  - Cuboids will use the learned morphology models to perform inference
  - After cells are classified as glia, they (90%) of the cells can be filtered out

[[./images/cmn.png]]

What this brings into Cuboids:
- Chunkflow
- TensorFlow (which is also used by Google's FFN)
- Post-reconstruction classification

A second generation of this would involve using the Allens CTDB to classify each IDs neuron by type.

** Lambda layers for image processing Python libraries
   
In the case of the Boss, the raw image data resides in AWS S3 and is
indexed in DynamoDB. In 2015 Lambda could not accommodate code
packages of a size that would result from including the machine
learning tools, say, scikit-learn. This is another valid reason why image
processing was performed external to the Boss. Currently though with
things like Lambda layers and TensorFlow Light it can be done.
Additionally, AWS Lambda -- The core technology
of serverless -- has mature in the interim such that large Python
codebases can now be deployed on Lambda. By adopting [[https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html][Lambda layers]] 3D
image recognition can be performed within a bossDB-based system, which
was not possible in 2015.

[[https://medium.com/@adhorn/getting-started-with-aws-lambda-layers-for-python-6e10b1f9a5d][Getting started with AWS Lambda Layers for Python]]

** High resolution billing

- Cuboids aims to have multiple users from multiple organizations.
- Users can initially be anonymous but through Cognito become authenticated.
  - For the anonymous users, resource usage costs need to be assigned to the hosted dataset.
  - Authenticated users will be Reconstrue users, probably members of organizations
  - Grafting on exist userbases is possible with Cognito. What about mapping that to billing?
- Fortunately, Lambda makes multi-tenancy easy [[https://narrativescience.com/resource/blog/how-aws-lambda-changed-the-game-of-multi-tenancy/][{1}]] [[https://techcrunch.com/2016/09/01/serverless-is-the-new-multitenancy/][{2}]]

* Open questions

- [[https://github.com/jhuapl-boss/boss-manage/blob/master/docs/InstallGuide.md#configure-scalyr-account][Scalyer for logging?]]
  - Surely this can be replaced at this time?
- What to do about high-resolution billing so the costs can be billed
  to specific organizations and departments.
  - Since this in to be an all-in with AWS project the system should
    probably be instrumented for AWS Marketplace.
- Validate guesses about cross VPC communications speeds
- Perhaps bossDB can be reduced to a single repository containing a full
  CloudFormation stack which deploys everything needed. For example, the
  RESTful API machinery is currently in a separate repo but if migrated
  to Amazon API Gateway, can be reduced to a single module in the
  serverless repo. It would be neat and clean to be able to spin up a
  serverless spatial DB as single CloudFormation. Cuboids itself will
  probably be a separate CloudFormation. But it is to early to predict
  this. First active coding needs to proceed and then revisit this.
- API Gateway was [[https://aws.amazon.com/about-aws/whats-new/2015/07/introducing-amazon-api-gateway/][introduced in 2015-07]]. The repo for the boss REST API was [[https://github.com/jhuapl-boss/boss/graphs/contributors][started in 2015-11]]. So not sure why it was not adopted. ]


* Conclusion

It would be very valuable and desirable to not cause a fork in the
community but it would appear that such may be inevitable. Although the main thrust of
this proposal is simply an architectural refresh without significant changes to what the
software does, these changes touch just about every component what with the modification
of the auth system to Cognito. Nonetheless, a eye will be kept out for areas where interaction
can be maintained. For example, perhaps two bossDB deploys could interact. Both would be
on AWS so the data would not need to travel far. This could be how JHU/APL's deploy would
be the primary repository for the MICrONS data yet others could deploy applications which
interact via de facto standard (Boss) APIs.

The main the assumptions of this argument is that the data and code
will never leave AWS. So, Step Functions is the API to various
components. One component is like ChuckFlow but as a lambda. Chuckflow
would need to be extended work with TensorFlow. Sectarian arguments
about TensorFlow versus PyTorch and their ilk are interesting but a
political goal of Cuboids is to bring the work of Google Connectomics
into the mix. Unsurprisingly, their work is based on TensorFlow so
TensorFlow it is.


With regards to such potential experiments, they can be build upon the existing Lambdas.


Point is: go all-in with AWS. Yuch, but a realtechnik practicality, sadly, for now.

* License
  :PROPERTIES:
  :UNNUMBERED: notoc
  :END:
This document is licensed under [[https://creativecommons.org/publicdomain/zero/1.0/][the CC0 1.0 Universal (CC0 1.0) Public Domain Dedication]]

To the extent possible under law, John Tigue has waived all
copyright and related or neighboring rights to Cuboids Manifesto. This
work is published from: United States.

John Tigue has dedicated this work to the public domain by waiving all
of his or her rights to the work worldwide under copyright law,
including all related and neighboring rights, to the extent allowed by
law.

You can copy, modify, distribute and perform the work, even for
commercial purposes, all without asking permission.

* References
  :PROPERTIES:
  :UNNUMBERED: notoc
  :END:
- BossDB preprints
  - Hider, Kleissas, et alia, 2019
    - [[https://www.biorxiv.org/content/10.1101/217745v2][The Block Object Storage Service (bossDB): A Cloud-Native Approach for Petascale Neuroscience Discovery]]
    - doi: https://doi.org/10.1101/217745
  - Kleissas, Hider, et alia, 2017
    - [[https://www.biorxiv.org/content/10.1101/217745v1.abstract][The Block Object Storage Service (bossDB): A Cloud-Native Approach for Petascale Neuroscience Discovery]]
    - doi: https://doi.org/10.1101/217745
- Dean Kleissas talks
  - [[https://youtu.be/ldNqVmW9c98][AWS re:Invent 2017: The Boss: A Petascale Database for Large-Scale Neuroscience, Pow (DAT401)]]
  - [[https://www.youtube.com/watch?v=806a3x2s0CY][The Boss: A Petascale DB for Large-Scale Neuroscience Powered by Serverless Advanced Technologies]]
- Functional imaging montage assembled from:
  - [[https://www.researchgate.net/publication/47300810_Functional_imaging_of_hippocampal_place_cells_at_cellular_resolution_during_virtual_navigation][Functional imaging of hippocampal place cells at cellular resolution during virtual navigation]]
  - [[https://www.biorxiv.org/content/10.1101/459941v1.full][In vivo widefield calcium imaging of the mouse cortex for analysis of network connectivity in health and brain disease]]
  - [[https://www.cell.com/neuron/supplemental/S0896-6273(07)00614-9][Imaging Large-Scale Neural Activity with Cellular Resolution in Awake, Mobile Mice]]
  - [[https://www.sciencedirect.com/science/article/pii/S221112471631676X][Long-Term Optical Access to an Estimated One Million Neurons in the Live Mouse Cortex]]
  - [[https://www.phenosys.com/products/virtual-reality/jetball-tft/][JetBall-TFT]]
  - [[https://www.slideshare.net/InsideScientific/mobile-homecage-ssneurotar][Making Optical and Electrophysiological Measurements in the Brain of Head-Fixed, Freely-Moving Rodents]]
- AWS technologies
  - [[https://aws.amazon.com/blogs/developer/aws-tech-talk-infrastructure-is-code-with-the-aws-cdk/][AWS Tech Talk: Infrastructure is Code with the AWS CDK]] AWS Developer Blog 2019-08
- Kool-Aid
  - The image in the introduction is a hacked up Marvel image, found via [[https://vsbattles.fandom.com/wiki/Kool-Aid_Man_(Marvel_Comics)][fandom.com]]
  - [[https://www.youtube.com/watch?v=_fjEViOF4JE][Kool-Aid Pitcher Man wall breaks]]
  - [[https://qz.com/74138/new-watered-down-kool-aid-man-just-wants-to-be-loved/][New, watered-down Kool-Aid Man just wants to be loved]]
- Mohammed and the Mountain cartoon
  - [[https://www.reddit.com/r/pics/comments/d07mf/look_gary_larson_put_mohammed_in_a_comic_and/][Far Side, Larson, 1992]]
